name: Benchmark CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  benchmark-smoke:
    name: Smoke Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - uses: actions/checkout@v3

    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        profile: minimal
        toolchain: stable
        override: true

    - name: Cache cargo registry
      uses: actions/cache@v3
      with:
        path: ~/.cargo/registry
        key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-registry-

    - name: Cache cargo index
      uses: actions/cache@v3
      with:
        path: ~/.cargo/git
        key: ${{ runner.os }}-cargo-index-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-index-

    - name: Cache cargo build
      uses: actions/cache@v3
      with:
        path: target
        key: ${{ runner.os }}-cargo-build-target-${{ hashFiles('**/Cargo.lock') }}
        restore-keys: |
          ${{ runner.os }}-cargo-build-target-

    - name: Build release binary
      run: cargo build --release -p nova_poc

    - name: Install Python dependencies
      run: |
        python3 -m pip install --upgrade pip
        pip install psutil pandas matplotlib

    - name: Run smoke benchmark (smallest preset)
      run: |
        python3 bench/bench.py \
          --grid K=4096 \
          --tile-k 1024 \
          --rounds 8 \
          --threads 1 \
          --modes one_pass \
          --repeats 1 \
          --out bench/ci_results.csv

    - name: Verify results
      run: |
        # Check that CSV was created and has expected rows
        if [ ! -f bench/ci_results.csv ]; then
          echo "‚ùå Benchmark failed: No output CSV"
          exit 1
        fi

        # Count rows (should be 3: infer, prove, verify)
        rows=$(tail -n +2 bench/ci_results.csv | wc -l)
        if [ "$rows" -ne 3 ]; then
          echo "‚ùå Unexpected row count: $rows (expected 3)"
          exit 1
        fi

        # Check for failures
        failures=$(grep -c ',False,' bench/ci_results.csv || true)
        if [ "$failures" -gt 0 ]; then
          echo "‚ùå Found $failures failed stages"
          cat bench/ci_results.csv
          exit 1
        fi

        echo "‚úÖ Smoke benchmark passed"

    - name: Check proof size constraint
      run: |
        # Extract proof_bytes and publics_bytes columns (adjust column numbers as needed)
        proof_bytes=$(tail -n +2 bench/ci_results.csv | cut -d',' -f23 | grep -v '^$' | head -1)
        publics_bytes=$(tail -n +2 bench/ci_results.csv | cut -d',' -f24 | grep -v '^$' | head -1)

        if [ -n "$proof_bytes" ] && [ -n "$publics_bytes" ]; then
          total_bytes=$((proof_bytes + publics_bytes))
          echo "Proof size: $proof_bytes bytes"
          echo "Public inputs: $publics_bytes bytes"
          echo "Total transaction: $total_bytes bytes"

          if [ "$total_bytes" -gt 1024 ]; then
            echo "‚ùå Transaction exceeds 1KB limit: $total_bytes bytes"
            exit 1
          fi

          echo "‚úÖ Transaction size under 1KB limit"
        else
          echo "‚ö†Ô∏è  Could not verify proof sizes"
        fi

    - name: Generate plots (optional)
      continue-on-error: true
      run: |
        python3 bench/plot_bench.py --in bench/ci_results.csv --outdir bench/ci_plots

    - name: Upload benchmark results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: |
          bench/ci_results.csv
          bench/ci_plots/
        retention-days: 30

    - name: Comment PR with results (if PR)
      if: github.event_name == 'pull_request'
      continue-on-error: true
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const csv = fs.readFileSync('bench/ci_results.csv', 'utf8');
          const lines = csv.split('\n');
          const header = lines[0].split(',');

          // Extract timing for each stage
          let timings = [];
          for (let i = 1; i < lines.length && i <= 3; i++) {
            const values = lines[i].split(',');
            const stage = values[header.indexOf('stage')];
            const wall_s = values[header.indexOf('wall_s')];
            if (stage && wall_s) {
              timings.push(`${stage}: ${wall_s}s`);
            }
          }

          const comment = `## üìä Benchmark Results

          **Configuration**: K=4096, tile_k=1024, rounds=8

          **Timings**:
          ${timings.join('\n')}

          ‚úÖ All stages completed successfully
          ‚úÖ Proof size under 1KB limit`;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  # Optional: Full benchmark on schedule or manual trigger
  benchmark-full:
    name: Full Benchmark Suite
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    steps:
    - uses: actions/checkout@v3

    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        profile: minimal
        toolchain: stable
        override: true

    - name: Build release binary
      run: cargo build --release -p nova_poc

    - name: Install Python dependencies
      run: |
        python3 -m pip install --upgrade pip
        pip install psutil pandas matplotlib

    - name: Run full benchmark grid
      run: |
        python3 bench/bench.py \
          --grid K=4096,12288,16384 \
          --tile-k 1024,4096 \
          --rounds 8,16 \
          --threads 1,auto \
          --modes one_pass \
          --repeats 2 \
          --out bench/full_results.csv

    - name: Generate plots
      run: |
        python3 bench/plot_bench.py --in bench/full_results.csv --outdir bench/full_plots

    - name: Upload full results
      uses: actions/upload-artifact@v3
      with:
        name: full-benchmark-results
        path: |
          bench/full_results.csv
          bench/full_plots/
          bench/full_plots/benchmark_summary.txt
        retention-days: 90